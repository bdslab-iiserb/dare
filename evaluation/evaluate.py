"""
Evaluation script for the Dialectical RAG Framework.

This script reads a results file generated by 'run_experiment.py' and
computes the final accuracy based on the strict Exact Match (EM) metric
established by prior work (MADAM-RAG). This ensures a direct, fair, and
reproducible comparison with the baseline.
"""
import argparse
import json
import re
import string
from pathlib import Path
import logging
from typing import List

# Configure basic logging for clear console output.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def normalize_answer(s: str) -> str:
    """
    Performs standard normalization on a string to prepare it for comparison.
    This involves lowercasing, removing punctuation, articles, and extra whitespace.
    """
    def remove_articles(text: str) -> str:
        return re.sub(r'\b(a|an|the)\b', ' ', text)

    def white_space_fix(text: str) -> str:
        return ' '.join(text.split())

    def remove_punc(text: str) -> str:
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text: str) -> str:
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def evaluate_single_entry(model_answer: str, gold_answers: List[str], wrong_answers: List[str]) -> bool:
    """
    Evaluates a single model answer against the ground truth using the strict EM metric.

    The strict EM metric requires two conditions to be met simultaneously:
    1.  Completeness: The model's answer must contain ALL of the gold answers.
    2.  Precision: The model's answer must contain NONE of the known wrong answers.

    Args:
        model_answer: The final answer generated by our framework.
        gold_answers: A list of all correct answers for the query.
        wrong_answers: A list of known incorrect answers for the query.

    Returns:
        True if the answer is correct according to the strict metric, False otherwise.
    """
    # An empty or non-string answer is always incorrect.
    if not model_answer or not isinstance(model_answer, str):
        return False

    norm_model_answer = normalize_answer(model_answer)
    norm_gold_answers = [normalize_answer(ans) for ans in gold_answers if ans]
    norm_wrong_answers = [normalize_answer(ans) for ans in wrong_answers if ans]

    # --- Test 1: Completeness ---
    # This checks if every single required gold answer is present in the model's output.
    # The `all()` function ensures this is a strict requirement.
    is_complete = all(gold in norm_model_answer for gold in norm_gold_answers)
    
    # --- Test 2: Precision ---
    # This checks if any known wrong answer has appeared in the model's output.
    # The `not any()` pattern ensures this is a strict requirement.
    is_precise = not any(wrong in norm_model_answer for wrong in norm_wrong_answers)

    return is_complete and is_precise

def main(results_filepath: Path):
    """
    Main function to orchestrate the evaluation of a full results file.
    """
    if not results_filepath.exists():
        logging.error(f"Results file not found: {results_filepath}")
        return

    logging.info(f"--- Starting Evaluation for: {results_filepath.name} ---")

    total_entries = 0
    correct_count = 0
    failed_debates = 0

    # 1. Load the results file line by line.
    with open(results_filepath, 'r', encoding='utf-8') as f:
        for line in f:
            total_entries += 1
            try:
                result = json.loads(line)
                
                # Handle cases where the debate itself failed and produced an error.
                if "error" in result:
                    failed_debates += 1
                    continue

                # 2. Extract the necessary fields for evaluation.
                model_answer = result.get("final_answer_object", {}).get("final_answer", "")
                gold_answers = result.get("gold_answers", [])
                wrong_answers = result.get("wrong_answers", [])

                # 3. Evaluate the entry and increment the counter if correct.
                if evaluate_single_entry(model_answer, gold_answers, wrong_answers):
                    correct_count += 1

            except (json.JSONDecodeError, KeyError) as e:
                logging.warning(f"Skipping malformed result line #{total_entries}: {e}")
                failed_debates += 1
                continue
    
    if total_entries == 0:
        logging.warning("No entries found in the results file.")
        return

    # 4. Calculate and report the final scores.
    # Accuracy is calculated only on the set of successfully completed debates.
    successful_debates = total_entries - failed_debates
    if successful_debates > 0:
        accuracy = (correct_count / successful_debates) * 100
    else:
        accuracy = 0.0

    print("\n" + "="*50)
    logging.info("--- EVALUATION COMPLETE ---")
    logging.info(f"Total Entries Processed: {total_entries}")
    logging.info(f"Successful Debates:      {successful_debates}")
    logging.info(f"Failed Debates (Errors): {failed_debates}")
    logging.info(f"Correctly Answered:      {correct_count}")
    logging.info(f"Final Strict EM Accuracy: {accuracy:.2f}%")
    print("="*50)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Evaluate the results of a framework experiment using the strict EM metric."
    )
    parser.add_argument(
        "results_file",
        type=str,
        help="The path to the experiment results .jsonl file (e.g., '../results/ramdocs_results.jsonl')."
    )
    
    args = parser.parse_args()
    results_path = Path(args.results_file)
    
    main(results_path)
