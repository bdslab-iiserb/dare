import argparse
import json
import re
import string
from pathlib import Path
import logging

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def normalize_answer(s: str) -> str:
    """
    Lower text, remove punctuation, articles, and extra whitespace.
    This is a critical step to ensure fair comparison.
    """
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def evaluate_single_entry(model_answer: str, gold_answers: list, wrong_answers: list) -> bool:
    """
    Evaluates a single model answer against the ground truth based on strict criteria.

    Args:
        model_answer (str): The final answer generated by our framework.
        gold_answers (list): A list of all correct answers.
        wrong_answers (list): A list of known incorrect answers.

    Returns:
        bool: True if the answer is correct, False otherwise.
    """
    if not model_answer or not isinstance(model_answer, str):
        return False

    norm_model_answer = normalize_answer(model_answer)
    norm_gold_answers = [normalize_answer(ans) for ans in gold_answers]
    norm_wrong_answers = [normalize_answer(ans) for ans in wrong_answers]

    # Test 1: Completeness - ALL gold answers must be present.
    is_complete = all(gold in norm_model_answer for gold in norm_gold_answers)
    
    # Test 2: Precision - NO wrong answers should be present.
    is_precise = not any(wrong in norm_model_answer for wrong in norm_wrong_answers if wrong)

    return is_complete and is_precise

def main(results_filepath: Path):
    """
    Main function to load a results file and compute the overall accuracy.
    """
    if not results_filepath.exists():
        logging.error(f"Results file not found: {results_filepath}")
        return

    logging.info(f"--- Starting Evaluation for: {results_filepath.name} ---")

    total_entries = 0
    correct_count = 0
    failed_debates = 0

    with open(results_filepath, 'r', encoding='utf-8') as f:
        for line in f:
            total_entries += 1
            try:
                result = json.loads(line)
                
                # Check if the debate failed for this entry
                if "error" in result:
                    failed_debates += 1
                    continue

                model_answer = result.get("final_answer_object", {}).get("final_answer", "")
                gold_answers = result.get("gold_answers", [])
                wrong_answers = result.get("wrong_answers", [])

                if evaluate_single_entry(model_answer, gold_answers, wrong_answers):
                    correct_count += 1

            except (json.JSONDecodeError, KeyError) as e:
                logging.warning(f"Skipping malformed result line: {e}")
                failed_debates += 1
                continue
    
    if total_entries == 0:
        logging.warning("No entries found in the results file.")
        return

    # Calculate accuracy, avoiding division by zero
    # We calculate accuracy on the entries where the debate successfully completed.
    successful_debates = total_entries - failed_debates
    if successful_debates > 0:
        accuracy = (correct_count / successful_debates) * 100
    else:
        accuracy = 0.0

    print("\n" + "="*50)
    logging.info("--- EVALUATION COMPLETE ---")
    logging.info(f"Total Entries Processed: {total_entries}")
    logging.info(f"Successful Debates: {successful_debates}")
    logging.info(f"Failed Debates (Errors): {failed_debates}")
    logging.info(f"Correctly Answered: {correct_count}")
    logging.info(f"Final Accuracy: {accuracy:.2f}%")
    print("="*50)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the results of a framework experiment.")
    parser.add_argument(
        "results_file",
        type=str,
        help="The path to the experiment results .jsonl file."
    )
    
    args = parser.parse_args()
    results_path = Path(args.results_file)
    
    main(results_path)
